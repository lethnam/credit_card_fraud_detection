# Credit card fraud detection

The data set is collected by the Machine Learning Group at Universit√© Libre de Bruxelles and hosted on [Kaggle.com](https://www.kaggle.com/mlg-ulb/creditcardfraud). It contains the information on credit card transactions, the timing, and 28 principal components that provide the background information (due to confidentiality issues).

The data set is highly imbalanced: frauds account for only 0.17% of all transactions. As such, even though the data are labeled (fraud vs. non-fraud), I want to check if anomaly detection (unsupervised) algorithms can effectively detect the frauds or not.  In this regard, I compare the performances of [Isolation Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html), [Local Outlier Factor](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html) and [Elliptic Envelope](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.EllipticEnvelope.html) (similarly [here](https://scikit-learn.org/stable/auto_examples/plot_anomaly_comparison.html#sphx-glr-auto-examples-plot-anomaly-comparison-py)). If frauds are truly outliers/anomalies, these algorithms should do well. However, they perform quite badly, suggesting that there can be distinctive patterns in the occurence of frauds. (The exploratory data analysis also indicates certain patterns in the fraudulent transactions.)

Since the anomaly detection techniques do not perform so well, I turn to the supervised-learning models to classify fraud transactions. To deal with the acute imbalance in the data, I implement the resampling techniques to re-balance the training data set. I compare the performances with under-sampling ([RandomUnderSampler](https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.under_sampling.RandomUnderSampler.html)) vs. with over-sampling ([SMOTE](https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SMOTE.html)). The models with both resampling techniques perform equally well. But the under-sampling, which reduces the size of the training set, helps to fit the models much faster.